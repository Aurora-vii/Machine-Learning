{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aurora-vii/Machine-Learning/blob/main/001_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "714846bc-2f22-4052-a3d0-a4479bbb4cab",
      "metadata": {
        "id": "714846bc-2f22-4052-a3d0-a4479bbb4cab"
      },
      "source": [
        "# 1 定义\n",
        "简单来说，线性回归是通过一组数据预测现实结果。  \n",
        "\n",
        "线性回归是机器学习中最基本的预测模型之一，主要用于找出两个或多个变量之间的线性关系。其基本思想是通过找到一条直线（在二维空间中）或一个平面（在三维空间中），最好地拟合给定数据点。\n",
        "    \n",
        "$$ f: x \\in \\mathbb{R}^d \\rightarrow y \\in \\mathbb{R} $$\n",
        "\n",
        "## 1.1 简单线性回归\n",
        "数学中我们常用：y=ax+b来表示，其中x为自变量，y为因变量，a为斜率，b为截距。机器学习中我们使用：\n",
        "$$ y = \\beta_0 + \\beta_1 x + \\epsilon $$\n",
        "$$ \\beta_0 \\text{ - 截距（当自变量 \\( x = 0 \\) 时，因变量 \\( y \\) 的值。）} $$\n",
        "$$ \\beta_1 \\text{ - 斜率（表示自变量每增加一个单位，因变量的平均变化量）} $$\n",
        "$$ \\epsilon \\text{ - 误差项 (表示模型预测值与实际值之间的差异，通常假设其符合正态分布)}$$\n",
        "\n",
        "## 1.2 多元线性回归\n",
        "在现实世界的问题中，通常需要通过多个预测变量来预测目标变量，这就是多元线性回归：\n",
        "$$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n $$\n",
        "$$ 每个 x_i 都是一个预测因素，而每个 \\beta_i 是相应变量的回归系数，表示自变量对因变量的线性影响。 $$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c42d88c9-9ace-4e71-809b-0d58e9a8aa4e",
      "metadata": {
        "id": "c42d88c9-9ace-4e71-809b-0d58e9a8aa4e"
      },
      "source": [
        "#### 问答题\n",
        "##### **1. 什么是线性回归 // 线性回归的基本原理**\n",
        "线性回归是一种基本的统计和机器学习方法，用于分析两个或多个变量之间的关系，并通过构建一个线性方程来预测一个或多个自变量对因变量的影响。它的核心思想是找到最适合的数据点的直线，以便利用这个模型对新数据进行预测。\n",
        "\n",
        "##### **2. 线性回归的基本假设**\n",
        "**线性关系假设（Linearity）：** 自变量和因变量之间存在线性关系，及因变量可以通过自变量的线性组合来表示。  \n",
        "**独立性假设（Independence）：** 观测值之间以及误差项之间是相互独立的，意味着数据样本是独立同分布的。  \n",
        "**同方差性假设（Homoscedasticity）：** 在所有自变量的不同取值下，误差项的分布应该具有相同的波动范围。  \n",
        "**正态性假设（Normality）：** 误差项服从正态分布，即： $ \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$  \n",
        "**无多重共线性假设（No Multicollinearity）：** 自变量之间不存在高度线性关系。  \n",
        "**误差项的期望为零（Zero Mean of Errors）：** 误差项的期望值为零，即 $ E(\\epsilon) = 0 $。即，误差项的正负偏差在所有样本中是均匀分布的，正误差和负误差相互抵消，整体上没有系统性的偏离。\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3c915d2-ead8-4fb8-b598-63ddb6f0506e",
      "metadata": {
        "id": "b3c915d2-ead8-4fb8-b598-63ddb6f0506e"
      },
      "source": [
        "# 2 线性回归模型的数学表达与矩阵形式\n",
        "## 2.1 线性回归的模型展示\n",
        "$$ \\hat{y} = w_0 + w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n $$\n",
        "\n",
        "$$ \\hat{y} \\text{ : 预测值，即模型的输出}$$\n",
        "\n",
        "$$ x_1, x_2, \\ldots, x_n \\text{ : 自变量（特征）}$$\n",
        "\n",
        "$$ w_0 \\text{ : 截距（偏置项），表示当所有特征值为零时的预测值}$$\n",
        "\n",
        "$$ w_1, w_2, \\ldots, w_n\\ \\text  { : 回归系数（权重），表示各特征对预测值的影响程度}$$\n",
        "\n",
        "## 2.2 线性回归的矩阵表示\n",
        "\n",
        "$$ y = w^T x + b $$\n",
        "$$ y \\text { : n * 1 大小的矩阵}$$\n",
        "$$ x \\text { : n * d 大小的矩阵}$$\n",
        "$$ w \\text { : d * 1 大小的矩阵}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f663656-b4c0-4476-bd09-4f967decc5fe",
      "metadata": {
        "id": "5f663656-b4c0-4476-bd09-4f967decc5fe"
      },
      "source": [
        "# 3 损失函数（Loss Function）\n",
        "在线性回归中，损失函数是用于衡量模型预测值与实际值之间差异的函数。\n",
        "\n",
        "## 3.1 均方误差（Mean Squared Error, MSE）\n",
        "MSE通过计算每个样本的预测误差的平方，并取这些平方误差的平均值，来衡量模型预测值与实际值之间的差异。\n",
        "$$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
        "$$ n \\text{ : 样本数量}$$\n",
        "$$ y_i \\text{ : 第i个样本的实际值}$$\n",
        "$$ \\hat{y}_i \\text{ : 第i个样本的预测值}$$\n",
        "### 3.1.1 特点\n",
        "**非负性：** MSE 始终是非负的，因为平方值不可能是负数。  \n",
        "**对大误差敏感：** 由于误差被平方，MSE对较大的误差更加敏感。这意味着在模型训练过程中，MSE会推动模型去减少那些较大的预测误差。  \n",
        "**可微性：** MSE 是一个光滑的、可微的函数，这使得它非常适合用于梯度下降等优化算法。  \n",
        "**解释性：** MSE 的单位是原始数据单位的平方，这可能在解释时不太直观。为了解决这个问题，通常会使用均方根误差（RMSE），它是MSE的平方根，单位与原始数据一致，更容易解释。\n",
        "\n",
        "## 3.2 误差平方和（Sum of Squared Error）\n",
        "SSE 计算的是所有样本的预测误差的平方和。它是线性回归中常用的一个评估指标，也是最小二乘法（OLS）要最小化的目标函数。\n",
        "$$ \\text{SSE} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
        "$$ n \\text{ : 样本数量}$$\n",
        "$$ y_i \\text{ : 第i个样本的实际值}$$\n",
        "$$ \\hat{y}_i \\text{ : 第i个样本的预测值}$$\n",
        "$$ \\text{MSE} = \\frac{1}{n} \\times \\text{SSE}$$\n",
        "### 3.2.1 矩阵表示\n",
        "$$ \\text{SSE} = (Y - Xw)^T (Y-Xw)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf79d829-a6ee-42a6-9ebb-a41da6a190b3",
      "metadata": {
        "id": "bf79d829-a6ee-42a6-9ebb-a41da6a190b3"
      },
      "source": [
        "#### 问答题\n",
        "##### **1. **"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96727594-db3b-4088-893a-af934107c022",
      "metadata": {
        "id": "96727594-db3b-4088-893a-af934107c022"
      },
      "source": [
        "# 4 寻找最佳回归系数 - $w_i$\n",
        "通过最小化所有观测值与模型预测值之间的误差平方(SSE)和来确定回归系数。\n",
        "\n",
        "## 4.1 最小二乘法//最小化误差平方和(Ordinary Least Squares, OLS // Minimizing Sum of Squared Error)\n",
        "最小二乘法的主要目的是通过最小化误差平方和，找到最优的回归系数，使得模型能够最好地拟合数据，从而在统计上提供准确且无偏的参数估计，并在未来的数据预测中表现良好。\n",
        "$$ w_{\\text{best}} = (X^T X)^{-1} X^T Y$$\n",
        "\n",
        "### 4.1.1  推导过程\n",
        "$$ L(w) = \\frac{1}{2} (Y-Xw)^T(Y-Xw) $$\n",
        "\n",
        "$ 展开表达式： $\n",
        "\n",
        "$$ L(w) = Y^TY - Y^TXw - w^TX^TY + X^Tw^TXw $$\n",
        "\n",
        "$ \\text{合并同类项，由于} Y^TXw \\text{是一个标量，它等于它自己的转置} w^TX^TY \\text{，于是可以简化为：} $\n",
        "\n",
        "$$ L(w) = Y^TY - 2w^TX^TY + X^Tw^TXw $$\n",
        "\n",
        "$ \\text{此时损失函数是一个开口向上的抛物线，全局有且仅有一个最小值，通过对损失函数} L(w) \\text{求导，并将导数设为0，可以找到使损失函数最小的} w \\text{值。} $\n",
        "\n",
        "$ \\text{求导过程如下：}$\n",
        "\n",
        "$$ \\frac{\\partial L(w)}{\\partial w} = -2X^T Y + 2X^T X w $$\n",
        "\n",
        "$ \\text{设导数} \\frac{\\partial L(w)}{\\partial w} = 0 \\text{，可以得到：} $\n",
        "\n",
        "$$ -2X^T Y + 2X^T X w = 0 $$\n",
        "\n",
        "$$ X^T X w = X^T Y $$\n",
        "\n",
        "$$ w_{\\text{best}} = (X^T X)^{-1} X^T Y $$\n",
        "\n",
        "### 4.1.2 优点\n",
        "原理接单易懂，计算过程简单直观；\n",
        "\n",
        "有明确的解析解，不需要迭代求解。\n",
        "\n",
        "### 4.1.3 局限性\n",
        "对异常值敏感：异常值会对结果产生较大影响。\n",
        "    \n",
        "多重共线性问题：如果自变量之间存在高度的线性相关性（多重共线性），则 $ X^T X $ 可能接近奇异，意味着 $ X $的 列向量之间存在着很强的线性相关性/多重共线性问题。在这种情况下，矩阵 $ X^T X$ 的行列式接近于0，这使得 $ (X^TX)^{-1}$ 的数值计算变得非常不稳定，导致回归系数 $ \\beta $ 的估计可能具有很大的误差。\n",
        "\n",
        "## 4.2 梯度下降优化算法\n",
        "从一个初始的参数值（通常为随机值或零）开始，逐步沿着损失函数下降最快的方向（即梯度方向）更新回归系数，直到找到使损失函数达到最小值的参数。\n",
        "\n",
        "### 4.2.1 推导过程\n",
        "$$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
        "\n",
        "为了最小化 $ MSE $ 我们需要对每个回归系数 $ \\beta_j $计算损失函数的导数，然后沿着导数的负方向更新参数。\n",
        "\n",
        "首先，展开 $ MSE $ 函数：\n",
        "\n",
        "$$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_n x_{in}))^2 $$\n",
        "\n",
        "对 $ \\beta_j $ 求偏导：\n",
        "\n",
        "$$ \\frac{\\partial MSE}{\\partial \\beta_j} = \\frac{\\partial}{\\partial \\beta_j} [\\frac{1}{n} \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_n x_{in}))^2]$$\n",
        "\n",
        "首先对括号内的平方项 $ (y_i - (\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_n x_{in}))^2 $ 进行求导：\n",
        "\n",
        "另 $ u = y_i - \\hat{y}_i$，并对其求导可以得到：\n",
        "\n",
        "$$ \\frac{\\partial}{\\partial \\beta_j}(y_i - \\hat{y}_i)^2 = 2 u \\cdot \\frac{\\partial u}{\\partial \\beta_j} = 2 \\cdot (y_i - \\hat{y}_i) \\cdot \\frac{\\partial (y_i - \\hat{y}_i)}{\\partial \\beta_j} $$\n",
        "\n",
        "对 $ y_i - \\hat{y}_i $ 进行求导可以得到：\n",
        "\n",
        "$$ \\frac{\\partial (y_i - \\hat{y}_i)}{\\partial \\beta_j} = \\frac{\\partial}{\\partial \\beta_i} \\cdot (y_i - (\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_n x_{in})) = - x_{ij}$$\n",
        "\n",
        "因此对 $ MSE $ 求导后结果为：\n",
        "\n",
        "$$ \\frac{\\partial MSE}{\\partial \\beta_j} = \\frac{1}{n} \\sum_{i=1}^{n} 2 \\cdot (y_i - \\hat{y}_i) \\cdot (- x{ij})$$\n",
        "\n",
        "整理得到：\n",
        "$$ \\frac{\\partial MSE}{\\partial \\beta_j} = - \\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i) \\cdot x{ij}$$\n",
        "\n",
        "**梯度下降更新规则：**\n",
        "$$ \\beta_j \\leftarrow \\beta_j - \\alpha \\cdot \\frac{\\partial MSE}{\\partial \\beta_j} $$\n",
        "$$ \\alpha \\text{ : 学习率，控制每次更新的步长} $$\n",
        "$$ \\frac{\\partial MSE}{\\partial \\beta_j} \\text{ : 对} \\beta_j \\text{的梯度} $$\n",
        "\n",
        "### 4.2.2 分类\n",
        "\n",
        "#### 4.2.2.1 批量梯度下降（Batch Gradient Descent）\n",
        "**原理：** 每次迭代使用**整个训练数据集**计算损失函数的梯度，然后更新给所有参数。更新公式为：  \n",
        "\n",
        "$$ \\beta_j \\leftarrow \\beta_j - \\alpha \\cdot \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i) \\cdot x_{ij} $$  \n",
        "\n",
        "**优点：** 每次更新的方向是整个数据集的全局最优方向，因此收敛更加**稳定**。  \n",
        "**缺点：** **计算开销大**，特别是在处理大规模数据集时，每次迭代需要遍历整个数据集。  \n",
        "**适用场景：** 适用于数据集规模较小或计算资源充足的情况。\n",
        "\n",
        "#### 4.2.2.2 随机梯度下降（Stochastic Gradient Descent, SGD)\n",
        "**原理：** 每次迭代仅使用**一个样本**计算梯度，并立刻更新参数。更新公式为：  \n",
        "\n",
        "$$ \\beta_j \\leftarrow \\beta_j - \\alpha \\cdot (y_i - \\hat{y}_i) \\cdot x_{ij} $$\n",
        "\n",
        "**优点：** 计算效率高，每次迭代的**计算量小**，适合大规模数据集和在线学习；能够逃离局部最小值。\n",
        "**缺点：**  由于每次更新仅基于一个样本，收敛路径可能**不稳定，波动较大**，最终结果可能不如批量梯度下降稳定。\n",
        "**适用场景：** 适用场景：大规模数据集、在线学习、快速初步优化。\n",
        "\n",
        "#### 4.2.2.3 小批量梯度下降（Mini-batch Gradient Descent）\n",
        "**原理：** 每次迭代使用一小批样本量为“m”的样本（称为 mini-batch）来计算梯度，并更新参数。更新公式为：\n",
        "\n",
        "$$ \\beta_j \\leftarrow \\beta_j - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i) \\cdot x_{ij} $$\n",
        "\n",
        "**优点：** **计算效率较高**，同时梯度估计的噪声较小，收敛更加**稳定**。\n",
        "**缺点：**  选择合适的批量大小（mini-batch size）需要一些调试。\n",
        "**适用场景：** 在大规模数据集上训练深度学习模型时，特别常用。\n",
        "\n",
        "#### 4.2.2.4 动量法（Momentum）\n",
        "#### 4.2.2.5 AdaGrad\n",
        "#### 4.2.2.6 RMSProp\n",
        "#### 4.2.2.7 Adam（Adaptive Moment Estimation）"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40bc275b-fbca-44c4-8f56-0859974a4d3c",
      "metadata": {
        "id": "40bc275b-fbca-44c4-8f56-0859974a4d3c"
      },
      "source": [
        "# 5 多重共线性问题\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}